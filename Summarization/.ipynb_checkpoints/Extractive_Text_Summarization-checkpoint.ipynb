{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summary Tool\n",
    "\n",
    "There are two kinds of summaries - **\"Extractive\"**, and **\"Abstractive\"**.\n",
    "\n",
    "This tool is an **Extractive** summary tool; That means it simply selects the \"most important\" sentences from a body of text and returns them. Computers are dumb though - There's no guarantee that this is a good summary. In our case, this tool counts how often each word is included in the text, then assigns a weight based on how often a word is used. If a sentence uses commonly-used words often, it'll likely score higher, and be returned by this tool. There are some unintended consequences too: longer sentences will be ranked more highly than shorter sentences.\n",
    "\n",
    "The way humans summarize text, you're synthesizing new content based on input. That's an abstractive summary, and not how this tool works. If that's something you're interested in, I'd recommend looking into Google Research's PEGASUS model.\n",
    "\n",
    "Code borrowed from:\n",
    "https://stackabuse.com/text-summarization-with-nltk-in-python/\n",
    "\n",
    "Another reference:\n",
    "https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capabilities\n",
    "\n",
    "This tool can ingest the following file formats:\n",
    "- .txt\n",
    "- .pdf (text-based; this won't OCR anything)\n",
    "- .doc, .docx\n",
    "- Limited url support.\n",
    "    - The tool will try to scrape your target site, but will not return particularly helpful messages if the requests fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint\n",
    "import heapq\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import textract\n",
    "\n",
    "import docx # This is whacky, but it's how you import python-docx. \n",
    "#pip install docx will install the wrong thing though\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "\n",
    "import requests\n",
    "import boilerpy3\n",
    "from boilerpy3 import extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to add iPython widget support\n",
    "Commented out below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "# import ipywidgets as widgets\n",
    "\n",
    "# def interactiveBoxes(k, article):\n",
    "#     summarize(k, article)\n",
    "    \n",
    "# iplot = interact(interactiveBoxes, n = widgets.Text(value='20',\n",
    "#     placeholder='Type something', description='Total #:',disabled=False),\n",
    "#         article = widgets.Text(value='https://en.wikipedia.org/wiki/Abstract_(summary)',\n",
    "#     placeholder='Type something', description='Highlight:',disabled=False),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPDFtext(filename):\n",
    "    pdf_text = textract.process(filename)\n",
    "    if isinstance(pdf_text, (bytes, bytearray)):\n",
    "        pdf_text = pdf_text.decode(\"utf-8\")\n",
    "    \n",
    "    return pdf_text\n",
    "\n",
    "def getDocXtext(filename):\n",
    "    ## Dumps the text of your word doc\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return ' '.join(fullText)\n",
    "\n",
    "def getUrlText(url):\n",
    "#     scraped_data = urllib.request.urlopen(filename)\n",
    "#     article = scraped_data.read()\n",
    "\n",
    "#     parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "#     paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "#     webtext = \"\"\n",
    "\n",
    "#     for p in paragraphs:\n",
    "#         webtext += p.text\n",
    "#     return webtext\n",
    "\n",
    "    doc = extractor.get_doc_from_url(url)\n",
    "    return doc.content\n",
    "\n",
    "\n",
    "def getText(filename):\n",
    "    #Split the file once on a period, starting from the rear, then grab the last entry in the resultant list\n",
    "    filetype = filename.split(\".\",-1)[-1].lower()\n",
    "    article_text = \"\"\n",
    "    if filetype == \"pdf\":\n",
    "        try:\n",
    "            print(\"Looks like a PDF\")\n",
    "            article_text = getPDFtext(filename)\n",
    "        except:\n",
    "            print(\"\\t>>Couldn't grab text\")\n",
    "    elif filetype in [\"doc\", \"docx\"]:\n",
    "        try:\n",
    "            print(\"Looks like a Word Doc\")\n",
    "            article_text = getDocXtext(filename)\n",
    "        except:\n",
    "            print(\"\\t>>Couldn't grab text\")\n",
    "            \n",
    "    elif filetype == \"txt\":\n",
    "        try:\n",
    "            print(\"Looks like a .txt doc\")\n",
    "            article_text = open(filename, \"r\").read()\n",
    "        except:\n",
    "            print(\"\\t>>Couldn't grab text\")\n",
    "            \n",
    "    elif filename.startswith(\"http\"):\n",
    "        print(\"Looks like a link!\")\n",
    "        try:\n",
    "            article_text = getUrlText(filename)\n",
    "        except:\n",
    "            print(\">>Couldn't grab text\")\n",
    "    else:\n",
    "        print(\"\\t>>Not sure what kind of file that is!\")\n",
    "    print(f\"\\t>>{len(nltk.word_tokenize(article_text))} words\\n\\t>>{len(nltk.sent_tokenize(article_text))} sentences\")\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(k, article):\n",
    "    article_text = getText(article)\n",
    "    # Removing Square Brackets and Extra Spaces\n",
    "    article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "    article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "\n",
    "\n",
    "    # Removing special characters and digits\n",
    "    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "    \n",
    "#     formatted_article_text = article_text.replace(\"\\t\", ' ', formatted_article_text)\n",
    "        \n",
    "    sentence_list = nltk.sent_tokenize(article_text)\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_article_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "                \n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        \n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "                        \n",
    "    summary_sentences = heapq.nlargest(k, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    # print(summary)\n",
    "    \n",
    "#     return summary\n",
    "\n",
    "    for doc in nltk.sent_tokenize(summary):\n",
    "        print(\"â€¢ \"+doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(k, article)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPtests",
   "language": "python",
   "name": "nlptests"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
