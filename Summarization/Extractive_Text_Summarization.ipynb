{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive Summary Tool\n",
    "\n",
    "There are two kinds of summaries - **\"Extractive\"**, and **\"Abstractive\"**.\n",
    "\n",
    "This tool is an **Extractive** summary tool; That means it simply selects the \"most important\" sentences from a body of text and returns them. Computers are dumb though - There's no guarantee that this is a good summary. In our case, this tool counts how often each word is included in the text, then assigns a weight based on how often a word is used. If a sentence uses commonly-used words often, it'll likely score higher, and be returned by this tool. There are some unintended consequences too: longer sentences will be ranked more highly than shorter sentences.\n",
    "\n",
    "The way humans summarize text, you're synthesizing new content based on input. That's an abstractive summary, and not how this tool works. If that's something you're interested in, I'd recommend looking into Google Research's PEGASUS model.\n",
    "\n",
    "Code borrowed from:\n",
    "https://stackabuse.com/text-summarization-with-nltk-in-python/\n",
    "\n",
    "Another reference:\n",
    "https://towardsdatascience.com/understand-text-summarization-and-create-your-own-summarizer-in-python-b26a9f09fc70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capabilities\n",
    "\n",
    "This tool can ingest the following file formats:\n",
    "- .txt\n",
    "- .pdf (text-based; this won't OCR anything)\n",
    "- .doc, .docx\n",
    "- Limited url support.\n",
    "    - The tool will try to scrape your target site, but will not return particularly helpful messages if the requests fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pprint import pprint\n",
    "import heapq\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re\n",
    "import textract\n",
    "\n",
    "import docx # This is whacky, but it's how you import python-docx. \n",
    "#pip install docx will install the wrong thing though\n",
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "\n",
    "import requests\n",
    "import boilerpy3\n",
    "from boilerpy3 import extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to add iPython widget support\n",
    "Commented out below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "# import ipywidgets as widgets\n",
    "\n",
    "# def interactiveBoxes(k, article):\n",
    "#     summarize(k, article)\n",
    "    \n",
    "# iplot = interact(interactiveBoxes, n = widgets.Text(value='20',\n",
    "#     placeholder='Type something', description='Total #:',disabled=False),\n",
    "#         article = widgets.Text(value='https://en.wikipedia.org/wiki/Abstract_(summary)',\n",
    "#     placeholder='Type something', description='Highlight:',disabled=False),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPDFtext(filename):\n",
    "    pdf_text = textract.process(filename)\n",
    "    if isinstance(pdf_text, (bytes, bytearray)):\n",
    "        pdf_text = pdf_text.decode(\"utf-8\")\n",
    "    \n",
    "    return pdf_text\n",
    "\n",
    "def getDocXtext(filename):\n",
    "    ## Dumps the text of your word doc\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return ' '.join(fullText)\n",
    "\n",
    "def getUrlText(url):\n",
    "#     scraped_data = urllib.request.urlopen(filename)\n",
    "#     article = scraped_data.read()\n",
    "\n",
    "#     parsed_article = bs.BeautifulSoup(article,'lxml')\n",
    "\n",
    "#     paragraphs = parsed_article.find_all('p')\n",
    "\n",
    "#     webtext = \"\"\n",
    "\n",
    "#     for p in paragraphs:\n",
    "#         webtext += p.text\n",
    "#     return webtext\n",
    "\n",
    "    doc = extractor.get_doc_from_url(url)\n",
    "    return doc.content\n",
    "\n",
    "\n",
    "def getText(filename):\n",
    "    #Split the file once on a period, starting from the rear, then grab the last entry in the resultant list\n",
    "    filetype = filename.split(\".\",-1)[-1].lower()\n",
    "    article_text = \"\"\n",
    "    if filetype == \"pdf\":\n",
    "        try:\n",
    "            print(\"Looks like a PDF\")\n",
    "            article_text = getPDFtext(filename)\n",
    "        except:\n",
    "            print(\"\\t>>Couldn't grab text\")\n",
    "    elif filetype in [\"doc\", \"docx\"]:\n",
    "        try:\n",
    "            print(\"Looks like a Word Doc\")\n",
    "            article_text = getDocXtext(filename)\n",
    "        except:\n",
    "            print(\"\\t>>Couldn't grab text\")\n",
    "            \n",
    "    elif filetype == \"txt\":\n",
    "        try:\n",
    "            print(\"Looks like a .txt doc\")\n",
    "            article_text = open(filename, \"r\").read()\n",
    "        except:\n",
    "            print(\"\\t>>Couldn't grab text\")\n",
    "            \n",
    "    elif filename.startswith(\"http\"):\n",
    "        print(\"Looks like a link!\")\n",
    "        try:\n",
    "            article_text = getUrlText(filename)\n",
    "        except:\n",
    "            print(\">>Couldn't grab text\")\n",
    "    else:\n",
    "        print(\"\\t>>Not sure what kind of file that is!\")\n",
    "    print(f\"\\t>>{len(nltk.word_tokenize(article_text))} words\\n\\t>>{len(nltk.sent_tokenize(article_text))} sentences\")\n",
    "    return article_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(k, article):\n",
    "    article_text = getText(article)\n",
    "    # Removing Square Brackets and Extra Spaces\n",
    "    article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n",
    "    article_text = re.sub(r'\\s+', ' ', article_text)\n",
    "\n",
    "\n",
    "    # Removing special characters and digits\n",
    "    formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n",
    "    formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n",
    "    \n",
    "#     formatted_article_text = article_text.replace(\"\\t\", ' ', formatted_article_text)\n",
    "        \n",
    "    sentence_list = nltk.sent_tokenize(article_text)\n",
    "    \n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    word_frequencies = {}\n",
    "    for word in nltk.word_tokenize(formatted_article_text):\n",
    "        if word not in stopwords:\n",
    "            if word not in word_frequencies.keys():\n",
    "                word_frequencies[word] = 1\n",
    "            else:\n",
    "                word_frequencies[word] += 1\n",
    "                \n",
    "    maximum_frequncy = max(word_frequencies.values())\n",
    "\n",
    "    for word in word_frequencies.keys():\n",
    "        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n",
    "        \n",
    "    sentence_scores = {}\n",
    "    for sent in sentence_list:\n",
    "        for word in nltk.word_tokenize(sent.lower()):\n",
    "            if word in word_frequencies.keys():\n",
    "                if len(sent.split(' ')) < 30:\n",
    "                    if sent not in sentence_scores.keys():\n",
    "                        sentence_scores[sent] = word_frequencies[word]\n",
    "                    else:\n",
    "                        sentence_scores[sent] += word_frequencies[word]\n",
    "                        \n",
    "    summary_sentences = heapq.nlargest(k, sentence_scores, key=sentence_scores.get)\n",
    "\n",
    "    summary = ' '.join(summary_sentences)\n",
    "    # print(summary)\n",
    "    \n",
    "#     return summary\n",
    "\n",
    "    for doc in nltk.sent_tokenize(summary):\n",
    "        print(\"• \"+doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks like a PDF\n",
      "\t>>10171 words\n",
      "\t>>469 sentences\n",
      "• While the human and information dimensions of an operational environment impact all types of military operations they are particularly important to the conduct of information advantage activities.\n",
      "• (ADP 6-0) Information Dimension The information dimension is the intersection of the physical and human dimensions where individual minds interpret information from the physical dimension of an operational environment.\n",
      "• When organic and attached capabilities are insufficient to achieve required information advantage objectives, commanders may also request information effects from higher echelon army and joint forces.\n",
      "• These commanders and their information advantage staffs will focus on achieving information advantage objectives within their assigned areas of operation using organic and attached capabilities.\n",
      "• Army forces plan, prepare, execute, and assess military information warfare and information advantage activities in collaboration with joint forces and unified action partners as required.\n",
      "• Special operations forces, commanded through the Theater Special Operations Command (TSOC), often provide joint force commanders key capabilities to influence the information and human dimensions of an operational environment.\n",
      "• Army commanders and staffs will generally coordinate and interact with joint forces to facilitate military information warfare and information advantage activities.\n",
      "• When planning and conducting information advantage activities, commanders must not limit themselves to only those capabilities previously associated with information operations.\n",
      "• However, information advantage staffs at division and below must engage with higher echelon staffs as they develop and execute information advantage activities.\n",
      "• Commanders and staffs must also protect friendly information to preserve friendly advantages and to deny adversary attempts to use friendly information to gain positions of relative advantage.\n",
      "• Commanders conduct information advantage activities in varied operational environments during competition below the level of armed conflict, crisis, and armed conflict.\n",
      "• Commanders gain and maintain information advantage in part by conducting information advantage activities.\n",
      "• As with all military operations, a capability that might be suitable for one information advantage objective might be ill-suited to another information advantage objective.\n",
      "• These commanders and information advantage staffs will continue to coordinate information advantage objectives and needs with and through the combatant commander.\n",
      "• The key information advantage tasks that enhance decision making are— ⚫ Enhance understanding of the human and information dimensions of the operational environment.\n",
      "• Chapter 3 describes possible mixtures of capabilities that commanders might use to conduct information advantage activities during competition below armed conflict, during crisis, and in conflict.\n",
      "• Information advantage activities include both operations, actions, and activities that focus on friendly forces or actors as well as those that focus on neutral, adversary, and enemy actors.\n",
      "• Chapter 2 presents a doctrinal approach to information advantage through the conduct of information advantage activities.\n",
      "• The information advantage staff element supports the commander by providing enhanced understanding of the human and information dimensions of the operational environment.\n",
      "• Army forces account for the information environment and the electromagnetic spectrum through the physical, human, and information dimensions of an operational environment.\n"
     ]
    }
   ],
   "source": [
    "summarize(20, r\"D:\\Work\\Army\\TIOG\\Information Advantage_Expanded White Paper_v1.5_20210128 (1).pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLPtests",
   "language": "python",
   "name": "nlptests"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
